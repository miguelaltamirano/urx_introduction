{
 "cells": [
  {
   "cell_type": "code",
   "execution_count": 5,
   "id": "03e452f2",
   "metadata": {},
   "outputs": [],
   "source": [
    "'''\n",
    "Sample Command:-\n",
    "python generate_aruco_tags.py --id 24 --type DICT_5X5_100 -o tags/\n",
    "'''\n",
    "import numpy as np\n",
    "import argparse\n",
    "import cv2\n",
    "import sys\n",
    "import time\n",
    "from scipy.spatial.transform import Rotation as R\n",
    "\n",
    "from IPython.display import clear_output\n",
    "\n",
    "\n",
    "\n",
    "# for 1920, 1080\n",
    "intrinsic_mat = np.array([\n",
    " [1.16249999e+03, 0.00000000e+00, 9.72336017e+02],\n",
    " [0.00000000e+00, 1.15780995e+03, 5.65214826e+02],\n",
    " [0.00000000e+00, 0.00000000e+00, 1.00000000e+00]])\n",
    "\n",
    "dist_coeff = np.array([[0.085, -0.199, -0.00065, 0.00063, 0.0469]])\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 6,
   "id": "billion-cleaning",
   "metadata": {},
   "outputs": [],
   "source": [
    "# pip uninstall opencv-python-headless==4.2.0.34\n",
    "\n",
    "# pip3 uninstall opencv-python\n",
    "\n",
    "#pip3 uninstall opencv-contrib-python\n",
    "#pip3 install opencv-contrib-python\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "exposed-undergraduate",
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": 9,
   "id": "91b4004b",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "4_id_0_350.png\n"
     ]
    }
   ],
   "source": [
    "ARUCO_DICT = {\n",
    "\"DICT_4X4_50\": cv2.aruco.DICT_4X4_50,\n",
    "\"DICT_4X4_100\": cv2.aruco.DICT_4X4_100,\n",
    "\"DICT_4X4_250\": cv2.aruco.DICT_4X4_250,\n",
    "\"DICT_4X4_1000\": cv2.aruco.DICT_4X4_1000,\n",
    "\"DICT_5X5_50\": cv2.aruco.DICT_5X5_50,\n",
    "\"DICT_5X5_100\": cv2.aruco.DICT_5X5_100,\n",
    "\"DICT_5X5_250\": cv2.aruco.DICT_5X5_250,\n",
    "\"DICT_5X5_1000\": cv2.aruco.DICT_5X5_1000,\n",
    "\"DICT_6X6_50\": cv2.aruco.DICT_6X6_50,\n",
    "\"DICT_6X6_100\": cv2.aruco.DICT_6X6_100,\n",
    "\"DICT_6X6_250\": cv2.aruco.DICT_6X6_250,\n",
    "\"DICT_6X6_1000\": cv2.aruco.DICT_6X6_1000,\n",
    "\"DICT_7X7_50\": cv2.aruco.DICT_7X7_50,\n",
    "\"DICT_7X7_100\": cv2.aruco.DICT_7X7_100,\n",
    "\"DICT_7X7_250\": cv2.aruco.DICT_7X7_250,\n",
    "\"DICT_7X7_1000\": cv2.aruco.DICT_7X7_1000,\n",
    "\"DICT_ARUCO_ORIGINAL\": cv2.aruco.DICT_ARUCO_ORIGINAL,\n",
    "\"DICT_APRILTAG_16h5\": cv2.aruco.DICT_APRILTAG_16h5,\n",
    "\"DICT_APRILTAG_25h9\": cv2.aruco.DICT_APRILTAG_25h9,\n",
    "\"DICT_APRILTAG_36h10\": cv2.aruco.DICT_APRILTAG_36h10,\n",
    "\"DICT_APRILTAG_36h11\": cv2.aruco.DICT_APRILTAG_36h11\n",
    "}\n",
    "\n",
    "tag_size = 350\n",
    "id_ = 0\n",
    "type_ = cv2.aruco.DICT_5X5_50\n",
    "\n",
    "dictionary = cv2.aruco.getPredefinedDictionary(cv2.aruco.DICT_4X4_250)\n",
    "# parameters =  cv.aruco.DetectorParameters()\n",
    "# detector = cv.aruco.ArucoDetector(dictionary, parameters)\n",
    "\n",
    "\n",
    "# arucoDict = cv2.aruco.Dictionary_get(type_)\n",
    "\n",
    "tag = np.zeros((tag_size, tag_size, 1), dtype=\"uint8\")\n",
    "cv2.aruco.generateImageMarker(dictionary, id_, tag_size, tag, 1)\n",
    "\n",
    "# Save the tag generated\n",
    "tag_name = f'{type_}_id_{id_}_{tag_size}.png'\n",
    "print(tag_name)\n",
    "cv2.imwrite(tag_name, tag)\n",
    "cv2.imshow(\"ArUCo Tag\", tag)\n",
    "cv2.waitKey(0)\n",
    "cv2.destroyAllWindows()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "3edfef02",
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": 10,
   "id": "1b43169d",
   "metadata": {},
   "outputs": [],
   "source": [
    "def draw_corners(aruco_dict_type):\n",
    "    \n",
    "    parameters = cv2.aruco.DetectorParameters()\n",
    "    aruco_dict = cv2.aruco.getPredefinedDictionary(aruco_dict_type)\n",
    "#     aruco_dict = cv2.aruco.Dictionary_get(aruco_dict_type)\n",
    "    cap = cv2.VideoCapture(0)#, cv2.CAP_DSHOW)\n",
    "    # cap.set(3, 1920)\n",
    "    # cap.set(4, 1080)\n",
    "    if not cap.isOpened():\n",
    "        raise ValueError(\"Error: Could not open video stream.\")\n",
    "\n",
    "    while True:\n",
    "        ret, frame = cap.read()\n",
    "        if not ret:\n",
    "            print(\"Error: Could not read frame.\")\n",
    "            break\n",
    "        \n",
    "        # Convert to grayscale\n",
    "        cv2.imshow('ArUco Marker Detection', frame)\n",
    "        gray = cv2.cvtColor(frame, cv2.COLOR_BGR2GRAY)\n",
    "        \n",
    "\n",
    "        # Detect ArUco markers\n",
    "        corners, ids, rejected = cv2.aruco.detectMarkers(gray, aruco_dict, parameters=parameters)\n",
    "        \n",
    "\n",
    "        # If markers are detected, draw them\n",
    "        if ids is not None:\n",
    "            cv2.aruco.drawDetectedMarkers(frame, corners, ids)\n",
    "#             print(corners)\n",
    "\n",
    "        # Display the resulting frame\n",
    "        cv2.imshow('ArUco Marker Detection', frame)\n",
    "\n",
    "        # Break the loop on 'q' key press\n",
    "        if cv2.waitKey(1) & 0xFF == ord('q'):\n",
    "            break\n",
    "\n",
    "    # Release the capture and close windows\n",
    "    cap.release()\n",
    "    cv2.destroyAllWindows()\n",
    "\n",
    "    \n",
    "draw_corners(cv2.aruco.DICT_5X5_50)\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 2,
   "id": "23ecf63f",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "rvec:  [[[ 0.85821187  0.44875201  0.24918674]\n",
      "  [ 0.50958713 -0.68662571 -0.51852299]\n",
      "  [-0.06159021  0.57198494 -0.81794858]]]\n",
      "tvec:  [[[-0.36675454 -0.21423287  0.83205791]]]\n",
      "rvec:  [[[ 0.77869523  0.54923854  0.30326683]\n",
      "  [ 0.60388667 -0.52503598 -0.59971502]\n",
      "  [-0.1701606   0.65013402 -0.74052085]]]\n",
      "tvec:  [[[-0.41394046 -0.14342836  0.7474721 ]]]\n"
     ]
    }
   ],
   "source": [
    "def pose_esitmation(aruco_dict_type, matrix_coefficients, distortion_coefficients):\n",
    "    # Open the video stream (replace with file path if using a video file)\n",
    "    video = cv2.VideoCapture(0)#, cv2.CAP_DSHOW)  # 0 for webcam, or replace with video file path\n",
    "\n",
    "    # Define the ArUco dictionary to use\n",
    "#     aruco_dict = aruco.Dictionary_get(cv2.aruco.DICT_5X5_50)  # Choose an appropriate dictionary\n",
    "    parameters = cv2.aruco.DetectorParameters()\n",
    "\n",
    "    \n",
    "#     gray = cv2.cvtColor(frame, cv2.COLOR_BGR2GRAY)\n",
    "    aruco_dict = cv2.aruco.getPredefinedDictionary(aruco_dict_type)\n",
    "#     parameters = cv2.aruco.DetectorParameters_create()\n",
    "    \n",
    "    while True:\n",
    "        # Capture frame-by-frame\n",
    "        clear_output(wait=True)\n",
    "        ret, frame = video.read()\n",
    "        if not ret:\n",
    "            print(\"No video\")\n",
    "            break\n",
    "\n",
    "        # Convert to grayscale\n",
    "        gray = cv2.cvtColor(frame, cv2.COLOR_BGR2GRAY)\n",
    "\n",
    "        # Detect ArUco markers\n",
    "        corners, ids, rejected = cv2.aruco.detectMarkers(gray, aruco_dict, parameters=parameters)\n",
    "\n",
    "        # If markers are detected, draw them\n",
    "        if ids is not None:\n",
    "            for i in range(0, len(ids)):\n",
    "                cv2.aruco.drawDetectedMarkers(frame, corners, ids)\n",
    "                rvec, tvec, markerPoints = cv2.aruco.estimatePoseSingleMarkers(corners[i], 0.06, matrix_coefficients, distortion_coefficients)\n",
    "                # Draw a square around the markers\n",
    "                cv2.aruco.drawDetectedMarkers(frame, corners) \n",
    "\n",
    "                # Draw Axis\n",
    "                cv2.drawFrameAxes(frame, matrix_coefficients, distortion_coefficients, rvec, tvec, 0.06)\n",
    "                RR= R.from_rotvec(rvec[0])\n",
    "                rvec = np.array(RR.as_matrix())\n",
    "                print(\"rvec: \", rvec)\n",
    "                print(\"tvec: \", tvec)\n",
    "        # Display the resulting frame\n",
    "        cv2.imshow('ArUco Marker Detection', frame)\n",
    "\n",
    "        # Break the loop on 'q' key press\n",
    "        if cv2.waitKey(1) & 0xFF == ord('q'):\n",
    "            break\n",
    "\n",
    "    # Release the capture and close windows\n",
    "    video.release()\n",
    "    cv2.destroyAllWindows()\n",
    "\n",
    "    \n",
    "pose_esitmation(cv2.aruco.DICT_5X5_50,intrinsic_mat, dist_coeff)\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "444f348a",
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "ff39d4b5",
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "acc23ccc",
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "c975b56a",
   "metadata": {},
   "outputs": [],
   "source": [
    "%pip install ultralytics"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 4,
   "id": "802ac59d",
   "metadata": {},
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "/home/lab/.local/lib/python3.8/site-packages/torch/cuda/__init__.py:128: UserWarning: CUDA initialization: The NVIDIA driver on your system is too old (found version 11040). Please update your GPU driver by downloading and installing a new version from the URL: http://www.nvidia.com/Download/index.aspx Alternatively, go to: https://pytorch.org to install a PyTorch version that has been compiled with your version of the CUDA driver. (Triggered internally at ../c10/cuda/CUDAFunctions.cpp:108.)\n",
      "  return torch._C._cuda_getDeviceCount() > 0\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "\n",
      "0: 480x640 1 person, 2 cups, 1 tv, 1 laptop, 1480.4ms\n",
      "Speed: 7.8ms preprocess, 1480.4ms inference, 1.6ms postprocess per image at shape (1, 3, 480, 640)\n",
      "person\n",
      "x: 138 y: 140\n",
      "cup\n",
      "x: 354 y: 108\n",
      "cup\n",
      "x: 544 y: 116\n",
      "tv\n",
      "x: 282 y: 56\n",
      "laptop\n",
      "x: 319 y: 343\n",
      "\n",
      "0: 480x640 1 person, 2 cups, 1 tv, 1 laptop, 1154.7ms\n",
      "Speed: 1.5ms preprocess, 1154.7ms inference, 0.9ms postprocess per image at shape (1, 3, 480, 640)\n",
      "person\n",
      "x: 138 y: 140\n",
      "cup\n",
      "x: 354 y: 99\n",
      "cup\n",
      "x: 544 y: 116\n",
      "tv\n",
      "x: 282 y: 56\n",
      "laptop\n",
      "x: 319 y: 344\n",
      "\n",
      "0: 480x640 1 person, 2 cups, 1 tv, 1 laptop, 1 vase, 1249.5ms\n",
      "Speed: 29.7ms preprocess, 1249.5ms inference, 1.1ms postprocess per image at shape (1, 3, 480, 640)\n",
      "person\n",
      "x: 138 y: 140\n",
      "cup\n",
      "x: 354 y: 100\n",
      "cup\n",
      "x: 544 y: 116\n",
      "vase\n",
      "x: 544 y: 116\n",
      "tv\n",
      "x: 280 y: 56\n",
      "laptop\n",
      "x: 319 y: 343\n",
      "\n",
      "0: 480x640 1 person, 2 cups, 1 tv, 1 laptop, 1 book, 1 vase, 1305.6ms\n",
      "Speed: 28.3ms preprocess, 1305.6ms inference, 1.3ms postprocess per image at shape (1, 3, 480, 640)\n",
      "person\n",
      "x: 138 y: 140\n",
      "cup\n",
      "x: 354 y: 109\n",
      "cup\n",
      "x: 544 y: 116\n",
      "vase\n",
      "x: 544 y: 116\n",
      "tv\n",
      "x: 282 y: 55\n",
      "laptop\n",
      "x: 319 y: 343\n",
      "book\n",
      "x: 271 y: 203\n",
      "\n",
      "0: 480x640 1 person, 2 cups, 1 tv, 1 laptop, 1 vase, 1198.4ms\n",
      "Speed: 12.1ms preprocess, 1198.4ms inference, 0.9ms postprocess per image at shape (1, 3, 480, 640)\n",
      "person\n",
      "x: 138 y: 141\n",
      "cup\n",
      "x: 354 y: 106\n",
      "vase\n",
      "x: 544 y: 116\n",
      "cup\n",
      "x: 544 y: 115\n",
      "tv\n",
      "x: 281 y: 57\n",
      "laptop\n",
      "x: 319 y: 342\n",
      "\n",
      "0: 480x640 1 person, 2 cups, 1 tv, 1 laptop, 1 book, 1173.4ms\n",
      "Speed: 1.6ms preprocess, 1173.4ms inference, 27.1ms postprocess per image at shape (1, 3, 480, 640)\n",
      "person\n",
      "x: 137 y: 141\n",
      "cup\n",
      "x: 544 y: 116\n",
      "tv\n",
      "x: 265 y: 56\n",
      "cup\n",
      "x: 354 y: 99\n",
      "laptop\n",
      "x: 319 y: 343\n",
      "book\n",
      "x: 275 y: 203\n",
      "\n",
      "0: 480x640 1 person, 2 cups, 1 tv, 1 laptop, 1244.9ms\n",
      "Speed: 59.8ms preprocess, 1244.9ms inference, 0.9ms postprocess per image at shape (1, 3, 480, 640)\n",
      "person\n",
      "x: 124 y: 141\n",
      "cup\n",
      "x: 544 y: 116\n",
      "cup\n",
      "x: 354 y: 108\n",
      "tv\n",
      "x: 265 y: 55\n",
      "laptop\n",
      "x: 319 y: 342\n",
      "\n",
      "0: 480x640 1 person, 2 cups, 1 tv, 1 laptop, 1165.0ms\n",
      "Speed: 4.5ms preprocess, 1165.0ms inference, 1.3ms postprocess per image at shape (1, 3, 480, 640)\n",
      "person\n",
      "x: 123 y: 141\n",
      "cup\n",
      "x: 544 y: 116\n",
      "cup\n",
      "x: 354 y: 108\n",
      "tv\n",
      "x: 260 y: 53\n",
      "laptop\n",
      "x: 319 y: 342\n",
      "\n",
      "0: 480x640 2 cups, 2 tvs, 1 laptop, 1536.4ms\n",
      "Speed: 2.2ms preprocess, 1536.4ms inference, 1.3ms postprocess per image at shape (1, 3, 480, 640)\n",
      "laptop\n",
      "x: 183 y: 116\n",
      "cup\n",
      "x: 578 y: 94\n",
      "cup\n",
      "x: 446 y: 134\n",
      "tv\n",
      "x: 517 y: 64\n",
      "tv\n",
      "x: 28 y: 56\n",
      "\n",
      "0: 480x640 2 cups, 2 tvs, 1 laptop, 1 scissors, 1166.3ms\n",
      "Speed: 3.0ms preprocess, 1166.3ms inference, 0.9ms postprocess per image at shape (1, 3, 480, 640)\n",
      "laptop\n",
      "x: 182 y: 117\n",
      "cup\n",
      "x: 578 y: 94\n",
      "cup\n",
      "x: 446 y: 135\n",
      "tv\n",
      "x: 518 y: 61\n",
      "tv\n",
      "x: 28 y: 57\n",
      "scissors\n",
      "x: 482 y: 70\n",
      "\n",
      "0: 480x640 2 cups, 2 tvs, 1 laptop, 1 mouse, 1210.0ms\n",
      "Speed: 1.5ms preprocess, 1210.0ms inference, 1.0ms postprocess per image at shape (1, 3, 480, 640)\n",
      "laptop\n",
      "x: 161 y: 296\n",
      "tv\n",
      "x: 498 y: 110\n",
      "cup\n",
      "x: 482 y: 344\n",
      "cup\n",
      "x: 372 y: 308\n",
      "tv\n",
      "x: 78 y: 103\n",
      "mouse\n",
      "x: 252 y: 408\n",
      "\n",
      "0: 480x640 3 cups, 1 bowl, 2 tvs, 1 laptop, 1589.6ms\n",
      "Speed: 1.4ms preprocess, 1589.6ms inference, 1.2ms postprocess per image at shape (1, 3, 480, 640)\n",
      "tv\n",
      "x: 475 y: 168\n",
      "tv\n",
      "x: 78 y: 317\n",
      "cup\n",
      "x: 499 y: 430\n",
      "cup\n",
      "x: 403 y: 412\n",
      "laptop\n",
      "x: 203 y: 382\n",
      "cup\n",
      "x: 580 y: 428\n",
      "bowl\n",
      "x: 602 y: 469\n",
      "\n",
      "0: 480x640 3 cups, 3 tvs, 1 laptop, 1113.0ms\n",
      "Speed: 2.3ms preprocess, 1113.0ms inference, 1.3ms postprocess per image at shape (1, 3, 480, 640)\n",
      "tv\n",
      "x: 475 y: 169\n",
      "tv\n",
      "x: 73 y: 308\n",
      "cup\n",
      "x: 487 y: 434\n",
      "laptop\n",
      "x: 184 y: 382\n",
      "cup\n",
      "x: 390 y: 418\n",
      "tv\n",
      "x: 184 y: 382\n",
      "cup\n",
      "x: 572 y: 434\n"
     ]
    }
   ],
   "source": [
    "import cv2\n",
    "from ultralytics import YOLO\n",
    "import numpy as np\n",
    "\n",
    "model = YOLO('yolov8x.pt')\n",
    "cap = cv2.VideoCapture(0)#, cv2.CAP_DSHOW)\n",
    "# cap.set(3, 1920)\n",
    "# cap.set(4, 1080)\n",
    "if not cap.isOpened():\n",
    "    raise ValueError(\"Error: Could not open video stream.\")\n",
    "\n",
    "while True:\n",
    "    ret, frame = cap.read()\n",
    "    if not ret:\n",
    "        print(\"Error: Could not read frame.\")\n",
    "        break\n",
    "\n",
    "    results = model(frame)\n",
    "    current_detections = []\n",
    "\n",
    "    # Extract detections from YOLO results\n",
    "    for result in results:\n",
    "        for box in result.boxes:\n",
    "            # Extract bounding box coordinates\n",
    "            x1, y1, x2, y2 = map(int, box.xyxy[0].tolist())\n",
    "            # Calculate center of the bounding box\n",
    "            center_x = (x1 + x2) // 2\n",
    "            center_y = (y1 + y2) // 2\n",
    "            \n",
    "            \n",
    "            # Extract label and confidence score\n",
    "            label = model.names[int(box.cls[0])]\n",
    "            confidence = box.conf[0].item()\n",
    "            print(label)\n",
    "            print(\"x:\", center_x, \"y:\",center_y)\n",
    "\n",
    "\n",
    "            # Draw bounding box and label on the frame\n",
    "            cv2.rectangle(frame, (x1, y1), (x2, y2), (0, 255, 0), 2)\n",
    "            cv2.putText(frame, f\"{label} {confidence:.2f}\", (x1, y1 - 10),\n",
    "                        cv2.FONT_HERSHEY_SIMPLEX, 0.5, (0, 255, 0), 2)\n",
    "            cv2.circle(frame, (center_x, center_y), 5, (255, 0, 0), -1)  # Draw center point\n",
    "\n",
    "        # Show the frame with bounding boxes\n",
    "    cv2.imshow(\"YOLO Live Detection\", frame)\n",
    "\n",
    "    # Exit if 'q' is pressed\n",
    "    if cv2.waitKey(1) & 0xFF == ord('q'):\n",
    "        break\n",
    "\n",
    "cap.release()\n",
    "cv2.destroyAllWindows()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "stainless-necessity",
   "metadata": {},
   "outputs": [],
   "source": [
    "%pip install python3-opencv"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "second-seeker",
   "metadata": {},
   "outputs": [],
   "source": []
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.8.10"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 5
}
